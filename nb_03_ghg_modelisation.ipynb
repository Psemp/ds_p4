{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import RepeatedKFold, LeaveOneOut, GridSearchCV\n",
    "from sklearn import neighbors, metrics\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from models.linear_regressions import Linear_reg\n",
    "from scripts.model_actions import freeze_model\n",
    "\n",
    "load_dotenv()\n",
    "sns.color_palette('colorblind')\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "pc_dpi = int(os.getenv('DPI'))\n",
    "\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Tentative de modélisation et prédiction de la variable : Intensité d'émission de GàES</u>\n",
    "\n",
    "## <u>1 : Modélisations en prenant en compte la note Energy Star (E*)</u>\n",
    "### <u>1.1 : Régressions linéaires</u>\n",
    "\n",
    "## <u>2 : Étude de l'importance de la note Energy Star</u>\n",
    "### <u>2.1 : Feature importance via SHAP </u>\n",
    "### <u>2.2 : Modélisation sans utiliser de variables E*</u>\n",
    "\n",
    "## <u>3 : Application d'une méthode non-linéaire</u>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>1 : Modélisations prenant en compte toutes les variables retenues lors de l'étude</u>\n",
    "### <u>1.1 : Régressions linéaires</u>\n",
    "\n",
    "#### 1.1.a : Cross validation = Leave One Out\n",
    "- On utilise dans un premier temps toutes les variables retenues lors de l'analyse exploratoire.\n",
    "- Un split satisfaisant à déjà été trouvé et fixé, ces données viendront sur-écrire le split proposé par le modèle (ici la Classe) par défaut\n",
    "- On effectue 4 régressions (OLS, Ridge, Lasso et Elastic Net) avec les paramètres par défaut de la classe pour la validation croisée : Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_file = \"./data/seattle_std_scaled.csv\"  # Used as backup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ghg = pd.read_csv(general_file)\n",
    "df_ghg.set_index(\"OSEBuildingID\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ghg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ghg.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target : target_GHGEmissionsIntensity(kgCO2e/ft2) :\n",
    "\n",
    "droplist = [\n",
    "    \"scaled_GHGEmissionsIntensity(kgCO2e/ft2)\",  # Scaled target\n",
    "    \"target_SourceEUI(kWh/m2)\",  # not to scale\n",
    "    \"EnergyStarCert\",\n",
    "    ]\n",
    "\n",
    "df_model = df_ghg.drop(columns=droplist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_target = \"target_GHGEmissionsIntensity(kgCO2e/ft2)\"\n",
    "ghg_regression = Linear_reg(dataframe=df_model, target=ghg_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading known split, ids are unique building OSE id\n",
    "\n",
    "with open(\"./data/splits_ghg.json\", \"r\") as json_file:\n",
    "    splits = json.load(json_file)\n",
    "\n",
    "ids_train = splits[\"train\"]\n",
    "ids_test = splits[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overriding\n",
    "\n",
    "df_train_override = df_model[df_model.index.isin(ids_train)]\n",
    "df_test_override = df_model[df_model.index.isin(ids_test)]\n",
    "\n",
    "ghg_regression.force_split(\n",
    "    df_train_ovr=df_train_override,\n",
    "    df_test_ovr=df_test_override\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exécution : \n",
    "Paramètres : \n",
    "- Ridge = 0.1, 45, step 0.05\n",
    "- Elastic Net = Alpha = Alpha_ridge = 0.1, 45, step 0.05, default L1 ratio\n",
    "- Lasso Alpha = 0.01, 15, 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck default : \n",
    "ghg_regression.common_parameters[\"cv\"] = None\n",
    "#\n",
    "\n",
    "alphas_ridge = np.arange(0.1, 45, 0.05)\n",
    "alphas_elnet = alphas_ridge\n",
    "alphas_lasso = np.arange(0.01, 15, 0.04)\n",
    "\n",
    "ghg_regression.execute_all(\n",
    "    alphas_elnet=alphas_elnet,\n",
    "    alphas_ridge=alphas_ridge,\n",
    "    alphas_lasso=alphas_lasso\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### --> 1 Visualisation des alphas et leur pertinence, affichage des meilleurs hyperparamètre\n",
    "##### --> 2 Comparaison des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Elnet : \\n\")\n",
    "print(f\"Elastic Net L1 Ratio : {ghg_regression.elnet_cv.l1_ratio_}\")\n",
    "print(f\"Elastic Net best Alpha : {ghg_regression.elnet_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.elnet_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.elnet_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge: \\n\")\n",
    "print(f\"Ridge best Alpha : {ghg_regression.ridge_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.ridge_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.ridge_time_card.t_predict}\")\n",
    "\n",
    "ghg_regression.ridge_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LASSO: \\n\")\n",
    "print(f\"LASSO best Alpha : {ghg_regression.lasso_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.lasso_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.lasso_time_card.t_predict}\")\n",
    "\n",
    "ghg_regression.lasso_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_with_estar_l1out = ghg_regression.format_all_metrics()\n",
    "\n",
    "df_predictions_estar_l1out = ghg_regression.df_predictions\n",
    "\n",
    "metrics_with_estar_l1out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Ridge Model and its predictions : \n",
    "\n",
    "ridge_l1out_estar = ghg_regression.ridge_cv\n",
    "predictions_ridge_l1out_estar = ghg_regression.df_predictions[\"Ridge\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation 1.1.a : \n",
    "- Ridge semble être le plus prometteur dans ce cas avec un score R2 supérieur à 0.6 et un rapport test/test stable.\n",
    "- L'erreur est de .695 et .686 kg de CO2/m2 pour les données train/test, respectivement. Le plus bas de toutes les régressions\n",
    "- On suit particulièrement la régression Ridge lors du passage a la vérification via Kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_model(model=ghg_regression, save_file=True, file_path=\"./data/ghg_splits.json\")\n",
    "# uncomment to save train test split ids (OSEBuildingID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.b : Cross validation = RepeatedKFold\n",
    "- Mêmes paramètres que 1.1.a\n",
    "- On effectue 4 régressions (OLS, Ridge, Lasso et Elastic Net), mise à jour des paramètres par défaut de la classe pour la validation croisée : RepeatedKfold(30 fois : 10 splits, 3 exécutions)\n",
    "- On attend des temps de traitement nettement supérieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "ghg_regression.common_parameters[\"cv\"] = k_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re executing all with adjusted cross validation parameters :\n",
    "\n",
    "# alphas_ridge = np.arange(0.1, 45, 0.05)  <-- already assigned, commented as a reminder\n",
    "# alphas_elnet = alphas_ridge\n",
    "# alphas_lasso = np.arange(0.01, 15, 0.04)\n",
    "\n",
    "ghg_regression.execute_all(\n",
    "    alphas_elnet=alphas_elnet,\n",
    "    alphas_ridge=alphas_ridge,\n",
    "    alphas_lasso=alphas_lasso\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Elnet : \\n\")\n",
    "print(f\"Elastic Net L1 Ratio : {ghg_regression.elnet_cv.l1_ratio_}\")\n",
    "print(f\"Elastic Net best Alpha : {ghg_regression.elnet_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.elnet_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.elnet_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge: \\n\")\n",
    "print(f\"Ridge best Alpha : {ghg_regression.ridge_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.ridge_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.ridge_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LASSO: \\n\")\n",
    "print(f\"LASSO best Alpha : {ghg_regression.lasso_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.lasso_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.lasso_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_with_estar_rkfold = ghg_regression.format_all_metrics()\n",
    "\n",
    "df_predictions_estar_rkfold = ghg_regression.df_predictions\n",
    "\n",
    "print(\"Kfold (10*3) : \\n\")\n",
    "\n",
    "metrics_with_estar_rkfold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Leave One Out : \\n\")\n",
    "\n",
    "metrics_with_estar_l1out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations :\n",
    "- Étonnamment, la validation croisée avec les paramètres utilisés (10 folds répétés 3 fois) n'offre pas de gain dans le cas de Ridge, au contraire.\n",
    "- Les autres modèles de régression sont toujours moins performants, que ce soit au niveau de l'erreur ou du score R2, ainsi que la transition train/test\n",
    "- Ridge Leave One Out se montre, dans ce cas, le meilleur candidat pour modéliser l'intensité d'émissions de GàES\n",
    "- On utilise Ridge avec Leave One Out comme baseline.\n",
    "- Le temps de traitement en utilisant la méthode Leave One Out est également beaucoup plus rapide que RKfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverting to cv = None for l1out\n",
    "\n",
    "ghg_regression.common_parameters[\"cv\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_regression.use_ridge_cv(alphas=alphas_ridge)  # obj.df_predictions actualized and reset for shap\n",
    "\n",
    "ghg_regression.scatter_true_pred(regression_name=\"Ridge\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion, partie 1 : \n",
    "\n",
    "- Ridge semble être le modèle à la fois le plus précis et le plus consistant. La validation via Kfolds n'offre pas d'avantages mais des pertes, que ce soit en terme de précision ou de temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>2 : Etude de l'importance de la note Energy Star</u>\n",
    "\n",
    "### <u>2.1 : Feature importance via SHAP</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = ghg_regression.df_origin.drop(columns=[ghg_target]).to_numpy()\n",
    "X100 = shap.utils.sample(X_all, 100) # 100 instances for use as the background distribution\n",
    "\n",
    "features = ghg_regression.df_origin.drop(columns=[ghg_target]).columns\n",
    "\n",
    "explainer = shap.LinearExplainer(ghg_regression.ridge_cv, X100, feature_names=features)\n",
    "shap_values = explainer(X_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ind = 18\n",
    "my_waterfall = shap.plots.waterfall(shap_values[0], max_display=33, show=False)\n",
    "my_waterfall.figure.set_size_inches(10, 10)\n",
    "my_waterfall.figure.set_dpi(pc_dpi)\n",
    "\n",
    "my_waterfall.suptitle(\"Impact des differentes variables sur le modele, classées par ordre d'importance\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = shap.plots.beeswarm(shap_values, max_display=32, show=False)\n",
    "\n",
    "summary.figure.set_size_inches(15, 8)\n",
    "summary.figure.set_dpi(pc_dpi)\n",
    "\n",
    "summary.suptitle(\"Impact des differentes variables sur le modele, classées par ordre d'importance\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations :\n",
    "\n",
    "Dans le cas de la modélisation de l'intensité d'émission de GàES, l'impact du score ENERGYSTAR ne semble pas avoir d'importance. En effet, les graphes ci dessus montrent que la variable influe extrêmement peu sur la sortie du modèle : <b>Waterfall n1 : index = 33, < .01 || Summary n2 : index = 32, impact extrêmement faible</b> <br>\n",
    "Ici, on peut émettre l'hypothèse que ne pas utiliser E* n'appauvrirait que très peu nos modèles <br>\n",
    "Le modèle sélectionné à l'issue de la première partie de cette analyse était Ridge en utilisant un système de validation Leave One Out. <br>\n",
    "On peut adapter la classe pour qu'elle abandonne le score E* et voir comment cela se traduit au niveau du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>2.2 : Modélisation sans utiliser de variables E*</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_estar = [\"ENERGYSTARScore\"]\n",
    "ghg_regression.drop_col(col_list=drop_estar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking ENERGYSTARScore not in matrices : shape should be 1 less than original dfs\n",
    "\n",
    "if ghg_regression.X_test.shape[1] >= len(ghg_regression.df_train.columns) + 1:  # +1 because target still in cols\n",
    "    print(\"Oops\")\n",
    "else:\n",
    "    print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.a : Cross validation = Leave One Out\n",
    "- Energy Star Score ne figure plus dans les matrices X_train, X_test\n",
    "- Mêmes paramètres initiaux que la régression comportant E*\n",
    "\n",
    "#### Exécution :\n",
    "Paramètres : \n",
    "- Ridge = 0.1, 45, step 0.05\n",
    "- Elastic Net = Alpha = Alpha_ridge = 0.1, 45, step 0.05, default L1 ratio\n",
    "- Lasso Alpha = 0.01, 15, 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_regression.execute_all(\n",
    "    alphas_elnet=alphas_elnet,\n",
    "    alphas_lasso=alphas_lasso,\n",
    "    alphas_ridge=alphas_ridge\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Elnet : \\n\")\n",
    "print(f\"Elastic Net L1 Ratio : {ghg_regression.elnet_cv.l1_ratio_}\")\n",
    "print(f\"Elastic Net best Alpha : {ghg_regression.elnet_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.elnet_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.elnet_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge: \\n\")\n",
    "print(f\"Ridge best Alpha : {ghg_regression.ridge_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.ridge_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.ridge_time_card.t_predict}\")\n",
    "\n",
    "ghg_regression.ridge_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LASSO: \\n\")\n",
    "print(f\"LASSO best Alpha : {ghg_regression.lasso_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.lasso_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.lasso_time_card.t_predict}\")\n",
    "\n",
    "ghg_regression.lasso_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_without_estar_l1out = ghg_regression.format_all_metrics()\n",
    "\n",
    "df_predictions_no_estar_l1out = ghg_regression.df_predictions\n",
    "\n",
    "metrics_without_estar_l1out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.b : Cross validation = RepeatedKfold\n",
    "- Mêmes paramètres que 2.2.a\n",
    "- On effectue 4 régressions (OLS, Ridge, Lasso et Elastic Net) mise à jour des paramètres par défaut de la classe pour la validation croisée : RepeatedKfold(30 fois : 10 splits, 3 exécutions)\n",
    "- On attend des temps de traitement nettement supérieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_regression.common_parameters[\"cv\"] = k_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_regression.execute_all(\n",
    "    alphas_elnet=alphas_elnet,\n",
    "    alphas_lasso=alphas_lasso,\n",
    "    alphas_ridge=alphas_ridge\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Elnet : \\n\")\n",
    "print(f\"Elastic Net L1 Ratio : {ghg_regression.elnet_cv.l1_ratio_}\")\n",
    "print(f\"Elastic Net best Alpha : {ghg_regression.elnet_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.elnet_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.elnet_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge: \\n\")\n",
    "print(f\"Ridge best Alpha : {ghg_regression.ridge_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.ridge_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.ridge_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LASSO: \\n\")\n",
    "print(f\"LASSO best Alpha : {ghg_regression.lasso_cv.alpha_}\")\n",
    "print(f\"Time used during fit = {ghg_regression.lasso_time_card.t_fit}\")\n",
    "print(f\"Time used during predict = {ghg_regression.lasso_time_card.t_predict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics using Leave One Out method, E* in model : \\n\")\n",
    "\n",
    "metrics_with_estar_l1out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_without_estar_kfold = ghg_regression.format_all_metrics()\n",
    "\n",
    "print(\"Metrics using repeated Kfold method (10, 3 repeats), E* not in model : \\n\")\n",
    "\n",
    "metrics_without_estar_kfold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics using Leave One Out method, E* in model : \\n\")\n",
    "\n",
    "metrics_with_estar_l1out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>3 : Utilisation d'une méthode non linéaire : régression via KNN</u>\n",
    "\n",
    "- Le problème semble être adapté à l'utilisation de méthodes de régressions linéaires\n",
    "- On peut vérifier cela en utilisant une méthode de modélisation non linéaire (ici régression par KNN) et comparer les performances des deux modèles (comparaison avec le meilleur modèle linéaire : Ridge).\n",
    "- On utilise exactement les mêmes données (provenant de la Classe Linear_reg), en gardant le même split.\n",
    "- On effectue deux validations croisées : Une fois en utilisant LeaveOneOut, et une autre fois en utilisant Kfolds (mêmes paramètres que pour les régressions linéaires)\n",
    "- On utilise la même métrique (MSE) pour la recherche d'hyperparamètre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghg_regression.reset_cols()  # We can reset the columns so that E* score is used\n",
    "\n",
    "X_train, X_test = ghg_regression.X_train, ghg_regression.X_test\n",
    "y_train, y_test = ghg_regression.y_train, ghg_regression.y_test\n",
    "\n",
    "neighbors_candidates = [5, 7, 9, 11, 13, 15, 17, 19]\n",
    "\n",
    "knnr = neighbors.KNeighborsRegressor()\n",
    "\n",
    "# Grid search\n",
    "\n",
    "l1out = LeaveOneOut()\n",
    "\n",
    "param_grid_knnr = {'n_neighbors':neighbors_candidates}\n",
    "\n",
    "score = \"neg_mean_squared_error\"\n",
    "\n",
    "# KNN regressors Setup\n",
    "\n",
    "knn_reg_l1o = GridSearchCV(\n",
    "    estimator=knnr,\n",
    "    param_grid=param_grid_knnr,\n",
    "    cv=l1out,\n",
    "    scoring=score\n",
    ")\n",
    "\n",
    "knn_reg_rkf = GridSearchCV(\n",
    "    estimator=knnr,\n",
    "    param_grid=param_grid_knnr,\n",
    "    cv=k_folds,\n",
    "    scoring=score\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnr_l1o_fit_ts = time.perf_counter()\n",
    "\n",
    "knn_reg_l1o.fit(\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "knnr_l1o_fit_tf = time.perf_counter()\n",
    "\n",
    "time_to_fit_knn_l1out = knnr_l1o_fit_tf - knnr_l1o_fit_ts\n",
    "\n",
    "rmse_knnr_l1o = np.sqrt(abs(knn_reg_l1o.best_score_))\n",
    "y_pred_train_l1o = knn_reg_l1o.predict(X_train)\n",
    "r2_knnr_l1o = metrics.r2_score(y_true=ghg_regression.y_train, y_pred=y_pred_train_l1o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE Regression KNN, Cross Val : Leave one out = {rmse_knnr_l1o}\")\n",
    "print(f\"Score R2 Regression KNN, Cross Val : Leave one out = {r2_knnr_l1o}\")\n",
    "print(f\"Temps fit : {time_to_fit_knn_l1out}\")\n",
    "print(f\"Meilleur hyperparametre : {knn_reg_l1o.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations, cas régression KNN, validation croisée Repeated Kfolds : \n",
    "- Le score R2 est beaucoup plus faible tous les autres scores obtenus lors de régressions linéaires\n",
    "- L'erreur augmente également significativement\n",
    "- Le temps de traitement pour fit (3.9s) est largement supérieur à celui de Ridge utilisant Leave One Out (0.071s)\n",
    "- On effectue la même expérimentation en utilisant la validation croisée plus rigoureuse : KFolds répétés (3 * 10 folds) pour avoir une conclusion définitive sur le modèle de régression par KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnr_rkf_fit_ts = time.perf_counter()\n",
    "\n",
    "knn_reg_rkf.fit(\n",
    "    X=X_train,\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "knnr_rkf_fit_tf = time.perf_counter()\n",
    "\n",
    "time_to_fit_knn_rkf = knnr_rkf_fit_tf - knnr_rkf_fit_ts\n",
    "\n",
    "rmse_knnr_rkf = np.sqrt(abs(knn_reg_rkf.best_score_))\n",
    "y_pred_train_rkf = knn_reg_rkf.predict(X_train)\n",
    "r2_knnr_rkf = metrics.r2_score(y_true=ghg_regression.y_train, y_pred=y_pred_train_l1o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE Regression KNN, Cross Val : Repeated KFolds = {rmse_knnr_rkf}\")\n",
    "print(f\"Score R2 Regression KNN, Cross Val : Repeated KFolds = {r2_knnr_rkf}\")\n",
    "print(f\"Temps fit : {time_to_fit_knn_rkf}\")\n",
    "print(f\"Meilleur hyperparametre : {knn_reg_rkf.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions :\n",
    "\n",
    "- Sur les modèles testés, le problème semble être plus aisément solvable en utilisant des méthodes de régressions linéaires (scores faibles en utilisant régression via KNN par ex.)\n",
    "\n",
    "L'hypothèse selon laquelle E* serait indispensable à la modélisation de prédictions <u>concernant les émissions de GàES</u> semble fausse. Le modèle Ridge est suffisamment robuste. L'analyse via SHAP et les comparaisons entre les différentes méthodes nous montrent que, vis à vis de nos données, l'utilisation d'une régression Ridge est optimale. La note E* apporte trop peu au modèle pour être considérée comme vitale. Les gains apportés ne valent pas les efforts demandés pour l'obtenir.\n",
    "\n",
    "- Recommandations issues de l'étude : Ridge utilisant comme hyperparamètre : 6.350000000000001\n",
    "- On pourrait étendre ce modèle pour améliorer sa précision en utilisant d'autres features catégorielles (Matériaux de construction, entreprise contractée pour les travaux, date de modernisation etc.)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1274e91d4c49da8d2c5027ac796fe23c6eef02ed77c0608f3ea1b98e1edcbe8a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
