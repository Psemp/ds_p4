{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from scipy import stats\n",
    "\n",
    "from models.linear_regressions import Linear_reg\n",
    "from scripts.frame_methods import scale_df, one_hot_dataframe\n",
    "\n",
    "load_dotenv()\n",
    "plt.style.use('Solarize_Light2')\n",
    "\n",
    "# Setting default DPI, pulling it from dotenv if it exists, setting it on 100 if not\n",
    "\n",
    "pc_dpi = int(os.getenv('DPI'))\n",
    "\n",
    "if pc_dpi is None:\n",
    "    pc_dpi = 100\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>micro note : I use hashes freely for titles or important cells in mkdown, even though it sometimes doesnt change the layout, it eases access using Outline from Vscode</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering :\n",
    "\n",
    "On cherche avant tout a maximiser l'efficacité du modèle en ajustant les données.On utilisera cela comme baseline `standard_scaler` de `sklearn` pour les benchmarks\n",
    "<br>\n",
    "Certaines features peuvent éventuellement être retouchées pour augmenter la précision de la modélisation. <br>\n",
    "\n",
    "- On peut analyser la répartition et la déviation des variables linéaires continues. Selon ces paramètres, on appliquera un Standard_scaler, un passage au log, à la racine carrée ou en utilisant la méthode de Box Cox\n",
    "- Les données concernant la taille des bâtiments (taille totale, taille bâtiments, taille parkings) peut être simplifiée : on peut utiliser la proportion de bâtiments et de parkings pour éviter les répétitions.\n",
    "- Les features catégorielles non ordinales doivent être encodée (sauf utilisation de CatBoost, hors de cette étude), on utilisera One Hot Encoder.\n",
    "- Conversion de \"Year Built\" vers \"Building Age\", cela devrait réduire le poids de cette variable (pour certains algorithmes) qui ne semblait pas avoir beaucoup d'influence sur les émissions de GàES ou la consommation énergétique durant l'analyse.\n",
    "- Ajout d'une nouvelle métrique booléenne : Energy_Star_Certified (E* >= 75). Cela peut être une alternative à l'élimination totale de cette variable au profit d'un calcul plus simple. L'étude de classification de cette feature est hors du spectre de l'étude mais pourrait fournir une piste pour obtenir plus facilement les données E* sans les calculer.\n",
    "<br>\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "\n",
    "- ## 1 - Modification des variables : Building Age, Proportions (parking / building), Certification E*\n",
    "- ## 2 - Analyse détaillée de la déviation et de la répartition des variables linéaires continues, application de la méthode optimale\n",
    "    - _note : on sauvegarde un dataset standardisé pour comparaisons_ <br><br>\n",
    "- ## 3 - Application de One Hot Encoder sur PrimaryPropertyType et Neighborhood\n",
    "    - On s'attend à un très grand nombre de de nouvelles colonnes, on applique donc cette modification en dernier <br><br>\n",
    "- ## 4 - Benchmark\n",
    "    - On compare les métriques de la régression du nouveau dataset avec celles de base (std_scaled)\n",
    "        - Si les métriques sont meilleure : On change de dataset\n",
    "        - Sinon : on utilise le dataset sauvegardé dans 2 _note_ (avec variable standardisées et mises à l'échelle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = \"./data/seattle_raw_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(raw_dataset)\n",
    "df_raw.set_index(\"OSEBuildingID\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"Neighborhood\"] = df_raw[\"Neighborhood\"].apply(str.upper)\n",
    "# It will make sense when we apply OHE and save a lot of repetitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 : Modifications des variables :\n",
    "\n",
    "- Âge\n",
    "- Proportion de bâtiments/parking\n",
    "- Certifié E* (booléen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 : Age des batiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"BuildingAge\"] = np.nan\n",
    "for index, row in df_raw.iterrows():\n",
    "    df_raw.loc[index, \"BuildingAge\"] = int(2022 - row[\"YearBuilt\"])\n",
    "\n",
    "df_raw.drop(columns=[\"YearBuilt\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "df_raw[\"BuildingAge\"].head()\n",
    "\n",
    "# cf. cell 5 (previous call of .head() method), comparison checks up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 : Ajout d'une nouvelle variable : Energy Star Certified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"EnergyStarCert\"] = 0\n",
    "for index, row in df_raw.iterrows():\n",
    "    if row[\"ENERGYSTARScore\"] >= 75:\n",
    "        df_raw.loc[index, \"EnergyStarCert\"] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check :\n",
    "\n",
    "df_raw[[\"EnergyStarCert\", \"ENERGYSTARScore\"]].head(n=10)\n",
    "\n",
    "# Coherent (0 if < 75, 1 if >= 75) , moving on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 : Modifications de variables : Proportion de parkings/batiments\n",
    "- Il est inutile de fournir la surface dédiée a chaque activité au modèle : `surface = proportion_Building*taille_totale + proportion_Parking*taille_totale`\n",
    "- L'utilisation d'un arrondi à 3 décimales causera dans certains cas `building_prop + parking_prop =/= 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"Parking_proportion\"] = np.nan\n",
    "df_raw[\"Building_proportion\"] = np.nan\n",
    "\n",
    "for index, series in df_raw.iterrows():\n",
    "    parking_prop = np.round(series[\"PropertyArea(SquareMetre)Parking\"] / series[\"PropertyArea(SquareMetre)Total\"], decimals=3)\n",
    "    building_prop = np.round(series[\"PropertyArea(SquareMetre)Building(s)\"] / series[\"PropertyArea(SquareMetre)Total\"], decimals=3)\n",
    "    df_raw.loc[index, \"Parking_proportion\"] = parking_prop\n",
    "    df_raw.loc[index, \"Building_proportion\"] = building_prop\n",
    "\n",
    "redundancies = [\"LargestPropertyUseTypeArea(SquareMetre)\", \"PropertyArea(SquareMetre)Parking\", \"PropertyArea(SquareMetre)Building(s)\"]\n",
    "\n",
    "df_raw = df_raw.drop(columns=redundancies, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check : (if propP+propB =/= 1 : oops)\n",
    "\n",
    "df_raw[[\"Parking_proportion\", \"Building_proportion\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Analyse détaillée de la déviation et de la répartition des variables linéaires continues, application de la méthode optimale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Standardisation et mise à l'échelle pour comparaison :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling : \n",
    "## Doubling targets : Scaled if in X_matrix, but not as target\n",
    "\n",
    "df_raw[\"target_GHGEmissionsIntensity(kgCO2e/ft2)\"] = df_raw[\"GHGEmissionsIntensity(kgCO2e/ft2)\"]\n",
    "df_raw[\"target_SourceEUI(kWh/m2)\"] = df_raw[\"SourceEUI(kWh/m2)\"]\n",
    "\n",
    "\n",
    "exempt_mod_cols = [\n",
    "    \"PrimaryPropertyType\", \"Neighborhood\", \"BuildingAge\", \"NumberofBuildings\", \"NumberofFloors\",\n",
    "    \"ENERGYSTARScore\", \"Building_proportion\", \"Parking_proportion\", \"EnergyStarCert\",\n",
    "    \"target_SourceEUI(kWh/m2)\", \"target_GHGEmissionsIntensity(kgCO2e/ft2)\"  # Ignore targets\n",
    "    ]\n",
    "\n",
    "df_std_scaled = scale_df(df_raw, constant_col=exempt_mod_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check : \n",
    "\n",
    "df_std_scaled.head(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset_std = [\n",
    "    \"scaled_Electricity(kWh)\", \"scaled_GHGEmissionsIntensity(kgCO2e/ft2)\", \n",
    "    \"scaled_PropertyArea(SquareMetre)Total\", \"scaled_SourceEUI(kWh/m2)\", \n",
    "    \"scaled_NaturalGas(kWh)\"\n",
    "    ]\n",
    "\n",
    "pairplot = sns.pairplot(\n",
    "    data=df_std_scaled[col_subset_std]\n",
    ")\n",
    "\n",
    "pairplot.figure.set_dpi(100)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "for axes in pairplot.axes.flat:\n",
    "    axes.set_ylabel(\"\")\n",
    "    axes.set_xlabel(axes.get_xlabel(), rotation=12)\n",
    "pairplot.figure.suptitle(\"\"\"Pairplot de distribution des variables\\\n",
    " lineaires continues, standardisation et mise à l'échelle\"\"\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations :\n",
    "\n",
    "- Source_EUI, PropertyArea, semblent être relativement normalement réparties\n",
    "- On enregistre la déviation des variables pour future comparaison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_std_scaled = dict.fromkeys(col_subset_std)\n",
    "for col in skew_std_scaled.keys():\n",
    "    skew_std_scaled[col] = df_std_scaled[col].skew()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copies des DataFrames\n",
    "-> utilisation de pandas.DataFrame.copy() évite les problèmes d'interdépendance qui pourraient survenir avec df1=df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 : Passage au log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df_raw.copy(deep=True)\n",
    "df_sqrt = df_raw.copy(deep=True)\n",
    "df_bcx = df_raw.copy(deep=True)\n",
    "\n",
    "col_subset = [\n",
    "    \"Electricity(kWh)\", \"GHGEmissionsIntensity(kgCO2e/ft2)\", \n",
    "    \"PropertyArea(SquareMetre)Total\", \"SourceEUI(kWh/m2)\", \"NaturalGas(kWh)\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 1 : \n",
    "\n",
    "for col in col_subset:\n",
    "    print((df_log[col] == 0).sum(), col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just add 1 to all zeroes in log_data since log(1) = 0: \n",
    "\n",
    "for col in col_subset:\n",
    "    df_log[col].replace(to_replace=0, value=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 2 : \n",
    "\n",
    "for col in col_subset:\n",
    "    print((df_log[col] == 0).sum(), col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = df_log[col_subset]\n",
    "\n",
    "for column in log_data.columns:\n",
    "    log_data[column] = np.log(log_data[column])\n",
    "\n",
    "pairplot = sns.pairplot(\n",
    "    data=log_data\n",
    ")\n",
    "\n",
    "pairplot.figure.set_dpi(100)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "for axes in pairplot.axes.flat:\n",
    "    axes.set_ylabel(\"\")\n",
    "    axes.set_xlabel(axes.get_xlabel(), rotation=12)\n",
    "pairplot.figure.suptitle(\"Pairplot de distribution des variables lineaires continues, passage au log\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations : \n",
    "- Il est compliqué d'évaluer l'effet sur la déviation à l'œil nu : enregistrement pour comparaison\n",
    "- Vérification nécessaire : df_raw ne contient pas de valeurs NaN, si des NaNs se manifestent, la cause est que la valeur est égale à 0\n",
    "    - log(0) = -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks : \n",
    "\n",
    "log_data.isna().sum()\n",
    "\n",
    "# Seems we have a winner, might be a park or something that doesnt use energy / doesnt emmit GHG, will fill with 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_logged = dict.fromkeys(col_subset)\n",
    "for col in skew_logged.keys():\n",
    "    skew_logged[col] = log_data[col].skew(skipna=True)\n",
    "\n",
    "skew_logged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remarque :\n",
    "\n",
    "- Le micro-fix consistant a remplacer les 0 par des 1 a fonctionné correctement\n",
    "- Enregistrement de la déviation pour comparaison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 : Passage à la racine carrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_data = df_sqrt[col_subset]\n",
    "\n",
    "for column in sqrt_data.columns:\n",
    "    sqrt_data[column] = np.sqrt(sqrt_data[column])\n",
    "\n",
    "pairplot = sns.pairplot(\n",
    "    data=sqrt_data\n",
    ")\n",
    "\n",
    "pairplot.figure.set_dpi(100)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "for axes in pairplot.axes.flat:\n",
    "    axes.set_ylabel(\"\")\n",
    "    axes.set_xlabel(axes.get_xlabel(), rotation=12)\n",
    "pairplot.figure.suptitle(\"Pairplot de distribution des variables lineaires continues, passage a la racine carrée\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations : Il semble encore une fois compliqué de conclure sur la distribution à l'oeil nu\n",
    "\n",
    "- On s'attend à voir deux résultats NaN : cf. hypothèse plus haut, on forcera le passage à 0\n",
    "- On enregistre les valeurs de déviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_sqrt = dict.fromkeys(col_subset)\n",
    "for col in skew_sqrt.keys():\n",
    "    skew_sqrt[col] = sqrt_data[col].skew()\n",
    "\n",
    "skew_sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks : \n",
    "\n",
    "sqrt_data.isna().sum()\n",
    "\n",
    "# Called it ! Most likely the same one. Will perform same check before outputting to csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 : Utilisation de la methode de Box Cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_cox_data = df_bcx[col_subset]\n",
    "\n",
    "for column in box_cox_data.columns:\n",
    "    values = box_cox_data[column].values\n",
    "    constant = abs(min(values)) + 0.000001\n",
    "    new_values = values + constant\n",
    "    new_values = stats.boxcox(new_values)\n",
    "    box_cox_data[column] = new_values[0]\n",
    "\n",
    "pairplot = sns.pairplot(\n",
    "    data=box_cox_data\n",
    ")\n",
    "\n",
    "pairplot.figure.set_dpi(100)\n",
    "\n",
    "###\n",
    "# Titles/Lables\n",
    "for axes in pairplot.axes.flat:\n",
    "    axes.set_ylabel(\"\")\n",
    "    axes.set_xlabel(axes.get_xlabel(), rotation=12)\n",
    "pairplot.figure.suptitle(\"Pairplot de distribution des variables lineaires continues, methode de Box Cox\")\n",
    "#\n",
    "###\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations : \n",
    "- Mêmes conclusions, on attend les chiffres pour procéder à une modification\n",
    "- Re vérification des valeurs NaNs (on devrait n'avoir aucun NaN du fait de l'ajout d'une constante --> `constant = abs(min(values)) + 0.000001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_box_cox = dict.fromkeys(col_subset)\n",
    "for col in skew_box_cox.keys():\n",
    "    skew_box_cox[col] = box_cox_data[col].skew()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check : \n",
    "\n",
    "box_cox_data.isna().sum()\n",
    "\n",
    "# Called it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name[7:] for name in skew_std_scaled.keys()]  # Removed \"scaled_\"\n",
    "results = pd.DataFrame(data=skew_std_scaled.values(), columns=[\"standard_scaled\"], index=names)\n",
    "results[\"log\"] = skew_logged.values()\n",
    "results[\"sqrt\"] = skew_sqrt.values()\n",
    "results[\"box_cox\"] = skew_box_cox.values()\n",
    "results = results.T  # Transformation as index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation de la valeur absolue minimale pour trouver la transformation adaptée à chaque variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n##############\\n\")\n",
    "\n",
    "for col in results.columns:\n",
    "    temp = abs(results[col])\n",
    "    print(f\"Minimal skew with {temp.idxmin()} for col {col[7:]} : skew = {results[col][temp.idxmin()]}\")\n",
    "\n",
    "print(\"\\n##############\\n\")\n",
    "\n",
    "for col in results.columns:\n",
    "    temp = abs(results[col].drop(index=[\"box_cox\"]))\n",
    "    print(f\" Minimal skew (without BCx) with {temp.idxmin()} for col {col[7:]} : skew = {results[col][temp.idxmin()]}\")\n",
    "\n",
    "print(\"\\n##############\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 : Application de One Hot Encoder sur PrimaryPropertyType et Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 : Application des transformations sur les variables lineaires continues :\n",
    "- 1 : Baseline en utilisant seulement standard_scaler\n",
    "- 2 : Déviation la plus faible (incluant la méthode de Box Cox)\n",
    "- 3 : Déviation la plus faible (exclusion de la méthode de Box Cox)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 : Standard scaler + OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_std_scaled is already modified, lets just use OHE and we will be fine with that\n",
    "\n",
    "ohe_subset = [\"PrimaryPropertyType\", \"Neighborhood\"]\n",
    "ohe_prefix = [\"Ptype\", \"Nbhood\"]\n",
    "\n",
    "df_std_scaled = one_hot_dataframe(\n",
    "    dataframe=df_std_scaled,\n",
    "    subset=ohe_subset,\n",
    "    prefix=ohe_prefix,\n",
    "    drop_og=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking : \n",
    "\n",
    "df_std_scaled.columns\n",
    "\n",
    "# ||| should be plenty of OHE and no PrimaryPropertyType & Neighbourhood (im bad at words)\n",
    "# |V| candidate 1 is ready, now for number 2 : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 : Candidat 2 (utilisant la méthode de Box Cox)\n",
    "- Minimal skew with box_cox for col Electricity(kWh) : skew = 0.14008000636298587\n",
    "- Minimal skew with box_cox for col GHGEmissionsIntensity(kgCO2e/ft2) : skew = 0.039485740356718965\n",
    "- Minimal skew with box_cox for col PropertyArea(SquareMetre)Total : skew = 0.1403831497723843\n",
    "- Minimal skew with sqrt for col SourceEUI(kWh/m2) : skew = -0.018985565524061002\n",
    "- Minimal skew with sqrt for col NaturalGas(kWh) : skew = 0.6566275254300923\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candidate_two = df_raw.copy(deep=True)\n",
    "\n",
    "change_to_bcx = [\n",
    "    \"Electricity(kWh)\", \"GHGEmissionsIntensity(kgCO2e/ft2)\", \"PropertyArea(SquareMetre)Total\"\n",
    "    ]\n",
    "\n",
    "change_to_sqrt = [\"SourceEUI(kWh/m2)\", \"NaturalGas(kWh)\"]\n",
    "\n",
    "\n",
    "for column in change_to_bcx:\n",
    "    values = df_candidate_two[column].values\n",
    "    constant = abs(min(values)) + 0.000001\n",
    "    new_values = values + constant\n",
    "    new_values = stats.boxcox(new_values)\n",
    "    df_candidate_two[column] = new_values[0]\n",
    "\n",
    "for column in change_to_sqrt:\n",
    "    df_candidate_two[column] = np.sqrt(df_candidate_two[column])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying OHE : \n",
    "\n",
    "df_candidate_two = one_hot_dataframe(\n",
    "    dataframe=df_candidate_two,\n",
    "    subset=ohe_subset,\n",
    "    prefix=ohe_prefix,\n",
    "    drop_og=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking : \n",
    "\n",
    "df_candidate_two.columns\n",
    "\n",
    "# ||| should be plenty of OHE and no PrimaryPropertyType & Neighbourhood (im bad at words)\n",
    "# |V| candidate 2 is ready, now for number 3 :\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 : Candidat 3 (en excluant la méthode de Box Cox)\n",
    "\n",
    "- Minimal skew (without BCx) with sqrt for col Electricity(kWh) : skew = 0.8793089997087102\n",
    "- Minimal skew (without BCx) with log for col GHGEmissionsIntensity(kgCO2e/ft2) : skew = -0.44455137612699647\n",
    "- Minimal skew (without BCx) with log for col PropertyArea(SquareMetre)Total : skew = 0.5898718071984281\n",
    "- Minimal skew (without BCx) with sqrt for col SourceEUI(kWh/m2) : skew = -0.018985565524061002\n",
    "- Minimal skew (without BCx) with sqrt for col NaturalGas(kWh) : skew = 0.6566275254300923\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_candidate_three = df_raw.copy(deep=True)\n",
    "\n",
    "change_to_log = [\"GHGEmissionsIntensity(kgCO2e/ft2)\", \"PropertyArea(SquareMetre)Total\"]\n",
    "\n",
    "change_to_sqrt = [\"SourceEUI(kWh/m2)\", \"NaturalGas(kWh)\", \"Electricity(kWh)\"]\n",
    "\n",
    "for col in change_to_log:\n",
    "    df_candidate_three[col].replace(to_replace=0, value=1, inplace=True)\n",
    "\n",
    "for column in change_to_log:\n",
    "    df_candidate_three[column] = np.log(df_candidate_three[column])\n",
    "\n",
    "for column in change_to_sqrt:\n",
    "    df_candidate_three[column] = np.sqrt(df_candidate_three[column])\n",
    "\n",
    "df_candidate_three.fillna(value=0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying OHE : \n",
    "\n",
    "df_candidate_three = one_hot_dataframe(\n",
    "    dataframe=df_candidate_three,\n",
    "    subset=ohe_subset,\n",
    "    prefix=ohe_prefix,\n",
    "    drop_og=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking : \n",
    "\n",
    "df_candidate_two.columns\n",
    "\n",
    "# ||| should be plenty of OHE and no PrimaryPropertyType & Neighbourhood (im bad at words)\n",
    "# |V|\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 : Benchmark sur emission de GaES\n",
    "- Kernel Fixé (via JSON)\n",
    "- Memes methodes, ajustement eventuel des parametres\n",
    "- Choix du meilleur dataset en fonction des meilleures metriques\n",
    "- Evaluation sur Ridge uniquement (modele lineaire le plus performant selon nos données)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chargement des indexes communs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading known split, ids are unique building OSE id\n",
    "\n",
    "with open(\"./data/splits_ghg.json\", \"r\") as json_file:\n",
    "    splits = json.load(json_file)\n",
    "\n",
    "ids_train = splits[\"train\"]\n",
    "ids_test = splits[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting targets & droplists : \n",
    "\n",
    "droplist_scaled = [\n",
    "    \"scaled_GHGEmissionsIntensity(kgCO2e/ft2)\",  # Scaled target\n",
    "    \"target_SourceEUI(kWh/m2)\",  # not to scale\n",
    "    \"EnergyStarCert\"\n",
    "    ]\n",
    "\n",
    "droplist_generic = [\n",
    "    \"GHGEmissionsIntensity(kgCO2e/ft2)\",  # Scaled target\n",
    "    \"target_SourceEUI(kWh/m2)\",  # not to scale\n",
    "    \"EnergyStarCert\"\n",
    "    ]\n",
    "\n",
    "target_ghg = \"target_GHGEmissionsIntensity(kgCO2e/ft2)\"\n",
    "\n",
    "df_candidate_one = df_std_scaled.drop(columns=droplist_scaled)\n",
    "df_candidate_two = df_candidate_two.drop(columns=droplist_generic)\n",
    "df_candidate_three = df_candidate_three.drop(columns=droplist_generic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation de trois objets de la classe Linear_reg :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_candidate_one = Linear_reg(dataframe=df_candidate_one, target=target_ghg)\n",
    "reg_candidate_two = Linear_reg(dataframe=df_candidate_two, target=target_ghg)\n",
    "reg_candidate_three = Linear_reg(dataframe=df_candidate_three, target=target_ghg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overriding random splits :\n",
    "# 1 : \n",
    "\n",
    "df_train_override_one = df_candidate_one[df_candidate_one.index.isin(ids_train)]\n",
    "df_test_override_one = df_candidate_one[df_candidate_one.index.isin(ids_test)]\n",
    "\n",
    "reg_candidate_one.force_split(\n",
    "    df_train_ovr=df_train_override_one,\n",
    "    df_test_ovr=df_test_override_one\n",
    ")\n",
    "\n",
    "###\n",
    "# 2 :\n",
    "\n",
    "df_train_override_two = df_candidate_two[df_candidate_two.index.isin(ids_train)]\n",
    "df_test_override_two = df_candidate_two[df_candidate_two.index.isin(ids_test)]\n",
    "\n",
    "reg_candidate_two.force_split(\n",
    "    df_train_ovr=df_train_override_two,\n",
    "    df_test_ovr=df_test_override_two\n",
    ")\n",
    "\n",
    "###\n",
    "# 3 :\n",
    "\n",
    "df_train_override_three = df_candidate_three[df_candidate_three.index.isin(ids_train)]\n",
    "df_test_override_three = df_candidate_three[df_candidate_three.index.isin(ids_test)]\n",
    "\n",
    "reg_candidate_three.force_split(\n",
    "    df_train_ovr=df_train_override_three,\n",
    "    df_test_ovr=df_test_override_three\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_ridge = np.arange(0.1, 45, 0.05)\n",
    "alphas_elnet = np.arange(42, 45, 1)  # We dont care about elnet, its the slowest and we wanna go fast\n",
    "alphas_lasso = np.arange(1, 2, .5) # Same same\n",
    "\n",
    "reg_candidate_one.execute_all(\n",
    "    alphas_elnet=alphas_elnet,\n",
    "    alphas_ridge=alphas_ridge,\n",
    "    alphas_lasso=alphas_lasso\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_candidate_one.ridge_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_one_perf = reg_candidate_one.ridge_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_ridge = np.arange(1600, 2800, 0.1)  # different data different inputs via trial/error \n",
    "alphas_elnet = np.arange(42, 45, 1)  # We dont care about elnet, its the slowest and we wanna go fast\n",
    "alphas_lasso = np.arange(1, 2, .5) # Same same\n",
    "\n",
    "reg_candidate_two.execute_all(\n",
    "    alphas_elnet=alphas_elnet,\n",
    "    alphas_ridge=alphas_ridge,\n",
    "    alphas_lasso=alphas_lasso\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_candidate_two.ridge_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_two_perf = reg_candidate_two.ridge_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_ridge = np.arange(5000, 6300, .1)  # different data different inputs via trial/error \n",
    "alphas_elnet = np.arange(42, 45, 1)  # We dont care about elnet, its the slowest and we wanna go fast\n",
    "alphas_lasso = np.arange(1, 2, .5) # Same same\n",
    "\n",
    "reg_candidate_three.execute_all(\n",
    "    alphas_elnet=alphas_elnet,\n",
    "    alphas_ridge=alphas_ridge,\n",
    "    alphas_lasso=alphas_lasso\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_candidate_three.ridge_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_three_perf = reg_candidate_three.ridge_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARING : \n",
    "print(\"Ridge perf, standard_scaled data :\")\n",
    "reg_one_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge perf, lowest skew, BCX included :\")\n",
    "reg_two_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ridge perf, lowest skew, BCX excluded :\")\n",
    "reg_three_perf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion & export : \n",
    "\n",
    "- A l'issue des benchmarks effectués sur les trois différentes méthodes de transformation de variables, l'utilisation de la standardisation/mise à l'échelle des variables linéaires continues se montre la plus précise sur la régression Ridge (score R2 plus haut et RMSE plus faible que les deux autres \"candidats\"). C'est cette modification qui sera exportée pour une étude plus approfondie.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Résumé des modifications : \n",
    "\n",
    "- Standardisation et mise à l'échelle des données linéaires continues\n",
    "- Création de nouvelles variables : E* certified, BuildingAge\n",
    "- Utilisation des proportions plutôt que des tailles (building+parking)\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Export des données pour analyses approfondies :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_scaled.to_csv(\"./data/seattle_std_scaled.csv\", sep=\",\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1274e91d4c49da8d2c5027ac796fe23c6eef02ed77c0608f3ea1b98e1edcbe8a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
